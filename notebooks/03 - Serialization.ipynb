{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to execute starlab commands and get the output in a format we can use, we need to think about\n",
    "storing that output in a permanent way. This process (taking objects in memory and writing them out to disk) is called *Serialization*.\n",
    "\n",
    "I've tidied up the `Story` class from the last notebook and placed it in a module for easy access. Before we can import and use it, though, we have to tell python where to find it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from pystarlab.starlab import Story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the code is worth looking at, so I'll use the `%load` magic to put it into a cell at the end of this notebook. For now, though, let's just build a list of commands and create a simulated star cluster. We'll go with a smallish cluster (500 stars) and integrate it for 100 nondimensional time units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmds = []\n",
    "\n",
    "cmds.append([\"makeking\", \"-n\", \"500\", \"-w\", \"5\", \"-i\",  \"-u\"])\n",
    "cmds.append([\"makemass\", \"-f\", \"2\", \"-l\", \"0.1,\", \"-u\", \"20\", \"-i\"])\n",
    "cmds.append([\"scale\", \"-m\", \"1\", \"-e\", \"-0.25\", \"-q\", \"0.5\"]) \n",
    "cmds.append([\"kira\", \"-t\", \"100\", \"-d\", \"1\", \"-D\", \"5\", \"-n\", \"10\", \"-q\", \"0.5\", \"-G\", \"2\"])\n",
    "\n",
    "storylist = Story.from_command_list(cmds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including the integration command in our command list gives us a list of stories instead of a single story. The two parameters which govern how many stories are in the list are `-t` (the amount of dynamical time in the integration) and `-D` (the interval between snapshots). In this case, `-t` is 100, and `-D` is 5, which should give us 20 snapshots. Including initial conditions (for t=0) brings the number up to 21. The `story_from_command_list()` method automatically includes the initial conditions if the end result is a list, so we don't need to do that manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(storylist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first goal here is to make a permanent archive of this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple serialization, two ways.\n",
    "\n",
    "The simplest serialization we could use is to just write the raw output of the `starlab` commands to a file (or collection of files) in the filesystem. Then, to recreate the `Story` objects, we would have to open and read the files, and convert the strings to `Story` objects. The `Story` class has a method to do this, so it's not a problem.\n",
    "\n",
    "A second approach would be to store the `Story` objects directly on disk. The standard python way to serialize objects is the [`pickle` module](https://docs.python.org/3.5/library/pickle.html). There are some kinds of things that can't be pickled, but we're not dealing with any of them here, so using `pickle` would be very straightforward.\n",
    "\n",
    "It seems like json would be a third sensible way to approach this, but the json library doesn't know what to do with our `Story` class. There are various things we could try to convert `Story` objects to json, but it doesn't seem worth it at this point. If we were going to do direct visualization of `Story` objects via the web, it might make sense, but when we get to visualization, we'll only be using a subset of the data. In any case, we should wait until we're ready to take that step before deciding how to do it.\n",
    "\n",
    "Let's compare the two obvious options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle\n",
    "\n",
    "To use `pickle`, we have to import it first. Then we just dump our list of `Story` objects to the file. The standard extension (inasmuch as there is one) for pickle files is `.pkl`.\n",
    "\n",
    "The construction here (`with open() ... `) automatically closes the file when we're done with it, and is more concise\n",
    "(and less error prone) than opening, writing, and closing with unconnected commands would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('kiraout.pkl', 'wb') as outfile:\n",
    "    pickle.dump(storylist, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plain text\n",
    "\n",
    "We don't have to import another module to write text to a file, but we do want to make sure we're using the\n",
    "right string representation of our `Story` objects, and we want to join the list into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('kira.out', 'w') as outfile:\n",
    "    outfile.write(\"\".join([str(story) for story in storylist]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gzipped text\n",
    "\n",
    "If we're going to use text, we might as well use the `gzip` module and compress it. It adds a little computational overhead, but saves us some space. The `encode()` method is necessary to turn the string into a `bytes` object for processing. People coming from python 2 may find this unintuitive (and annoying), but ultimately I think the other benefits of python 3 make it worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "with gzip.open('kira.out.gz', 'wb') as outfile:\n",
    "    outfile.write(\"\".join([str(story) for story in storylist]).encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do they compare, size-wise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 cmckay cmckay 4315695 Feb  8 20:38 kira.out\r\n",
      "-rw-rw-r-- 1 cmckay cmckay 1293300 Feb  8 20:38 kira.out.gz\r\n",
      "-rw-rw-r-- 1 cmckay cmckay 7434322 Feb  8 20:38 kiraout.pkl\r\n"
     ]
    }
   ],
   "source": [
    "! ls -l kira*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. The pickled version is almost twice as big as the plain text, and gzipped text is the obvious winner. Since we can trivially recreate the list from the plain text file, pickling doesn't seem to buy us anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "with open('kira.out', 'r') as infile:\n",
    "    new_story_list = Story.from_buf(infile)\n",
    "    \n",
    "print(len(new_story_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filesystem vs. Database\n",
    "\n",
    "Knowing how we're going to serialize the `Story` really only answers half of the question of how we're going to archive and store our data. The other half of the question is, where are we going to put it so that we can find it again?\n",
    "\n",
    "Ultimately, we'd like to be able to find simulations that satisfy different criteria (all King model simulations with 500 stars, for example). To accomplish this, we'll need some way to connect stories with their metadata. We'll explore this in more detail in the next notebook, but it's worth going through a few options here.\n",
    "\n",
    "The biggest question, though, is whether to store our data as files in the filesystem, or as entries in a database. There are advantages and disadvantages to each, so it's not immediately obvious which is the better choice. It's the kind of thing that gets discussed occasionally in discussion groups (see, for example, [this stackoverflow thread](http://stackoverflow.com/questions/504544/whats-the-best-practice-for-storing-huge-amounts-of-text-into-a-db-or-as-a-fil)) usually without clear resolution.\n",
    "\n",
    "The main virtue of the filesystem is simplicity, but that's also its main failing. For our application, it's useful to know that:\n",
    "\n",
    "- eventually we're going to be running lots of simulations in parallel across several different machines\n",
    "- individual snapshots will typically be smaller than 1.5 MB (2600 stars gives a size of about 1.3 MB per snapshot)\n",
    "- we will be running ensembles of simulations consisting of 100 runs or so.\n",
    "- this is a write-once, read-occasionally-for-ETL situation; when we're doing analytics or plotting, we will use slices or subsets of the data rather than these archives.\n",
    "- our aim is to build an interface to (at the very least) the metadata using Django.\n",
    "\n",
    "The last point here is probably the most important one; since we're going to be using Django for the web interface, we'll be using postgresql for that, and it makes sense to use it for archiving the data, as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "storylist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "storylist[0].story_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single value just tells us how many stars are in our cluster (or, more specifically, how many `Particle` stories are contained in the story tree below the root). Note that this *won't* be the same as the number of subobjects; we have a few subobjects that aren't `Particle`s, and if we have any combined `Particle`s (such as for binaries), they would show up as single subobjects but would all be counted for `N`. \n",
    "\n",
    "In this case, we don't have any binaries or other doubled up `Particle`s, so we're left with 500 `Particle` subobjects and 4 others.\n",
    "\n",
    "The first is the `Log` object. This is valuable for reproducibility and data provenance, but I'm not going to need to extract or modify any values in the near term, so it might as well stay as a string or list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(str(storylist[0].story_subobjects[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the root level `Dynamics` object. The most important things here are the center of mass position and velocity, and the system time. We will add some more important fields in subsequent snapshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(str(storylist[0].story_subobjects[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Hydro` story isn't relevant to any of the projects we've undertaken so far, so it's empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "storylist[0].story_subobjects[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, the `Star` object isn't, either, but this root level scaling could be useful if we're trying to communicate results in any kind of dimensional units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(str(storylist[0].story_subobjects[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, it's all `Particle`s. They unfortunately aren't sorted in obvious way. Presumably, they're sorted in some way that's convenient to whatever tool generated that particular snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(str(storylist[0].story_subobjects[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(str(storylist[0].story_subobjects[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snapshots from the integration\n",
    "\n",
    "Once `kira` is running, we add quite a bit more information, and some of it is useful.\n",
    "\n",
    "If we're looking at the subobjects in the same order, we will start with the `Log`, which now contains information\n",
    "about cpu time. This could be useful if we want to know which factors affect performance, and by how much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(str(storylist[1].story_subobjects[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dynamics` object now has energies, lagrangian radii, and a modified center of mass (which is also now the basis of the coordinate system)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(str(storylist[1].story_subobjects[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(str(storylist[1].story_subobjects[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(str(storylist[1].story_subobjects[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another wrinkle: as time passes, we lose some stars. Whatever serialization method we use will need to deal with this gracefully. We could turn it off (by not passing the `-G` option to `kira`) but it would be nice to be able to cope with it if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(storylist[3].story_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy dtypes\n",
    "\n",
    "We can make the data representation more compact (and index/sliceable) by using numpy dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this works by doing the simplest thing that will be useful. For the dynamics story of any particle, we want the mass, position, and velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dynamics_type = np.dtype([('mass', np.float64), ('r', np.float64, (3,)), ('v', np.float64, (3,))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually construct a variable of this type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_to_dynamics_type(particle_story):\n",
    "    \"\"\"extract the dynamical values from a particle story\"\"\"\n",
    "    dynamics_story = particle_story.story_subobjects[1]\n",
    "    mass = float(dynamics_story.story_vals['m'])\n",
    "    r = tuple(map(float, dynamics_story.story_vals['r'].split(\" \")))\n",
    "    v = tuple(map(float, dynamics_story.story_vals['v'].split(\" \")))\n",
    "    \n",
    "    return (mass, r, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "convert_to_dynamics_type(storylist[1].story_subobjects[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "column = np.array([convert_to_dynamics_type(particle) for particle in storylist[1].story_subobjects[5:]], dtype=dynamics_type)\n",
    "\n",
    "column[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next bit we need to add is sorting a list of `Particle`s by their name/number, and then using that as an index in the resulting array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def particle_id(particle_story):\n",
    "    return int(particle_story.story_vals['i'])\n",
    "\n",
    "particle_id(storylist[1].story_subobjects[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_particles = sorted(storylist[1].story_subobjects[5:], key=particle_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(str(sorted_particles[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the indices. Python uses zero indexing, so we'll have to subtract one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.array([particle_id(particle) for particle in sorted_particles]) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example we ran above, we have 21 time snapshots and 500 particles. Let's see if we can store the dynamics in a 500x21 array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize\n",
    "particle_dynamics = np.empty((500, 21))\n",
    "particle_dynamics[:,:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for time_index, time_snapshot in enumerate(storylist):\n",
    "    sorted_particles = sorted(time_snapshot.story_subobjects[4:], key=particle_id)\n",
    "    indices = np.array([particle_id(particle) for particle in sorted_particles]) - 1\n",
    "    column = np.array([convert_to_dynamics_type(particle) for particle in sorted_particles], dtype=dynamics_type)\n",
    "    particle_dynamics[indices, time_index] = column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_particles = sorted(storylist[7].story_subobjects[4:], key=particle_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "storylist[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(str(storylist[7].story_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_particle_tree(particle_story):\n",
    "    \"\"\"Flatten a tree of particle stories.\"\"\"\n",
    "    \n",
    "    # first, make sure something needs to be done\n",
    "    if int(particle_story.story_vals['N']) == len(particle_story.story_subobjects[4:]):\n",
    "        return particle_story\n",
    "    else:\n",
    "        #find the particles that contain more particles\n",
    "        for particle in particle_story.story_subobjects[4:]:\n",
    "            if  len(particle.story_subobjects) != 4:\n",
    "                # we will need to apply a transformation for center of mass\n",
    "                print(str(particle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flatten_particle_tree(storylist[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "int(storylist[5].story_vals['N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(storylist[5].story_subobjects[4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "storylist[4].story_subobjects[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved serialization with h5py\n",
    "\n",
    "Now that we know what we're dealing with, we can put together a schema for storing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = h5py.File('test.h5', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix:  The `Story` class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load \"../pystarlab/starlab.py\"\n",
    "from subprocess import Popen, PIPE\n",
    "import os\n",
    "import re\n",
    "from tempfile import SpooledTemporaryFile as tempfile\n",
    "\n",
    "class Story:\n",
    "    \"\"\"Generic container class for starlab data.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Create an empty story.\"\"\"\n",
    "        self.story_lines = []\n",
    "        self.story_vals = dict()\n",
    "        self.story_subobjects = []\n",
    "        self.kind = None\n",
    "        return\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"A unique representation of the story object.\"\"\"\n",
    "        return (\"[Story] %s, %d lines, %d values, %d subobjects\" %\n",
    "                (self.kind,\n",
    "                 len(self.story_lines),\n",
    "                 len(self.story_vals.keys()),\n",
    "                 len(self.story_subobjects)))\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"A string matching starlab's native format.\"\"\"\n",
    "        selfstr = \"(%s\\n\" % self.kind\n",
    "        for line in self.story_lines:\n",
    "            selfstr += \"%s\\n\" % line\n",
    "        for key, val in sorted(self.story_vals.items()):\n",
    "            selfstr += \"  %s = %s\\n\" % (key, val)\n",
    "        for substory in self.story_subobjects:\n",
    "            selfstr += str(substory)\n",
    "        return selfstr + \")%s\\n\" % self.kind\n",
    "\n",
    "    @classmethod\n",
    "    def from_buf(cls, buffered_result):\n",
    "        \"\"\"Generate a story from a buffer.\n",
    "\n",
    "        This could either be a stream or a string that has\n",
    "        been split into lines. It's supposed to add flexibility for\n",
    "        running long kira integrations in which we don't want to hold\n",
    "        the whole string in memory when converting it to stories.\n",
    "\n",
    "        We use a little bit of state to avoid using recursion here. The\n",
    "        reason for that is twofold:\n",
    "\n",
    "        1. We want to treat lines in Log-type stories a little differently, and\n",
    "        2. This will be more efficient, especially for large buffers.\n",
    "\n",
    "        :param buffered_result: Results of a starlab command in an iterable format\n",
    "        :type buffered_result: iterable\n",
    "\n",
    "        :returns: results parsed into a story\n",
    "        :rtype: story instance\n",
    "        \"\"\"\n",
    "        stories_to_return = []\n",
    "        story_stack = []\n",
    "\n",
    "        # shouldn't be necessary\n",
    "        thestory = None\n",
    "\n",
    "        for line in buffered_result:\n",
    "            if isinstance(line, bytes):\n",
    "                line = line.decode()\n",
    "            # check to see if we need to start a new story\n",
    "            storystart = re.match(\"^\\((\\w+)\",line)\n",
    "            if storystart:\n",
    "                thestory = cls()\n",
    "                thestory.kind = storystart.group(1)\n",
    "                story_stack.append(thestory)\n",
    "            else:\n",
    "                storyend = re.match(\"\\)%s\" % story_stack[-1].kind, line)\n",
    "                if storyend:\n",
    "                    thestory = story_stack.pop()\n",
    "                    if len(story_stack) > 0:\n",
    "                        story_stack[-1].story_subobjects.append(thestory)\n",
    "                    else:\n",
    "                        stories_to_return.append(thestory)\n",
    "                else:\n",
    "                    chunks = re.split('=', line)\n",
    "                    if ((len(chunks) == 2) and story_stack[-1].kind != \"Log\"):\n",
    "                        story_stack[-1].story_vals[chunks[0].strip()] = chunks[1].strip()\n",
    "                    else:\n",
    "                        story_stack[-1].story_lines.append(line)\n",
    "\n",
    "        if len(stories_to_return) == 0:\n",
    "            raise ValueError(\"No stories found in buffer!\")\n",
    "        elif len(stories_to_return) == 1:\n",
    "            return stories_to_return[0]\n",
    "        else:\n",
    "            return stories_to_return\n",
    "\n",
    "    @classmethod\n",
    "    def from_string(cls, result_string):\n",
    "        \"\"\"Generate a story from a string.\n",
    "\n",
    "        Assumes the string contains a single story (possibly with story subobjects).\n",
    "        If there's more than one story in the string (e.g., output from kira), this\n",
    "        will grab the last and discard the rest.\n",
    "\n",
    "        :param result_string: The string to parse\n",
    "        :type result_string: bytestring or unicode string\n",
    "\n",
    "        :returns: string parsed into a story\n",
    "        :rtype: Story instance\n",
    "        \"\"\"\n",
    "        if isinstance(result_string, bytes):\n",
    "            lines = result_string.decode('utf-8').splitlines()\n",
    "        elif isinstance(result_string, str):\n",
    "            lines = result_string.splitlines()\n",
    "        else:\n",
    "            raise TypeError('result_string should be a string or bytestring')\n",
    "\n",
    "        newstory = cls.from_buf(lines)\n",
    "\n",
    "        return newstory\n",
    "\n",
    "    @classmethod\n",
    "    def from_single_command(cls, command):\n",
    "        \"\"\"Generate a story from a single command.\n",
    "\n",
    "        The command should be a creation command (e.g., makeking, makeplummer, etc.).\n",
    "        It should also include all of the necessary command line options.\n",
    "\n",
    "        :param command: The starlab command to run\n",
    "        :type command: a string as it would appear on the command line\n",
    "                       or a list suitable for subprocess.Popen()\n",
    "\n",
    "        :returns: the output of command\n",
    "        :rtype: Story instance\n",
    "        \"\"\"\n",
    "        if isinstance(command, str):\n",
    "            command = command.split(\" \")\n",
    "        elif isinstance(command, list):\n",
    "            pass\n",
    "        else:\n",
    "            raise TypeError('command should be a string or list')\n",
    "        thestory = None\n",
    "        story_lines = []\n",
    "\n",
    "        with Popen(command, stdout=PIPE, bufsize=1, universal_newlines=True) as process:\n",
    "            for line in process.stdout:\n",
    "                story_lines.append(line.rstrip())\n",
    "\n",
    "        thestory = cls.from_buf(story_lines)\n",
    "\n",
    "        return thestory\n",
    "\n",
    "    @classmethod\n",
    "    def from_command_list(cls, command_list):\n",
    "        \"\"\"Generate a story from a list of commands.\"\"\"\n",
    "        current_story = cls.from_single_command(command_list.pop(0))\n",
    "        for command in command_list:\n",
    "            current_story = current_story.apply_command(command)\n",
    "        return current_story\n",
    "\n",
    "    def apply_command(self, command):\n",
    "        \"\"\"Apply a starlab command to this story and return the result\"\"\"\n",
    "        if isinstance(command, str):\n",
    "            command = command.split(\" \")\n",
    "        elif isinstance(command, list):\n",
    "            pass\n",
    "        else:\n",
    "            raise TypeError('command should be a string or list')\n",
    "\n",
    "        story_lines = []\n",
    "        with tempfile() as f:\n",
    "            f.write(str(self).encode())\n",
    "            f.seek(0)\n",
    "            with Popen(command, stdout=PIPE, stdin=f,\n",
    "                        universal_newlines=True, bufsize=1) as process:\n",
    "                for line in process.stdout:\n",
    "                    story_lines.append(line.rstrip())\n",
    "\n",
    "        thestory = self.from_buf(story_lines)\n",
    "\n",
    "        # if the command was an integration, we'll get a list\n",
    "        if isinstance(thestory, list):\n",
    "            # include the initial conditions\n",
    "            thestory.insert(0, self)\n",
    "        return thestory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
