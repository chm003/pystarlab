{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to execute starlab commands and get the output in a format we can use, we need to think about\n",
    "storing that output in a permanent way. This process (taking objects in memory and writing them out to disk) is called *Serialization*.\n",
    "\n",
    "I've tidied up the `Story` class from the last notebook and placed it in a module for easy access. Before we can import and use it, though, we have to tell python where to find it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from pystarlab.starlab import Story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the code is worth looking at, so I'll use the `%load` magic to put it into a cell at the end of this notebook. For now, though, let's just build a list of commands and create a simulated star cluster. We'll go with a smallish cluster (500 stars) and integrate it for 100 nondimensional time units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmds = []\n",
    "\n",
    "cmds.append([\"makeking\", \"-n\", \"500\", \"-w\", \"5\", \"-i\",  \"-u\"])\n",
    "cmds.append([\"makemass\", \"-f\", \"2\", \"-l\", \"0.1,\", \"-u\", \"20\", \"-i\"])\n",
    "cmds.append([\"scale\", \"-m\", \"1\", \"-e\", \"-0.25\", \"-q\", \"0.5\"]) \n",
    "cmds.append([\"kira\", \"-t\", \"100\", \"-d\", \"1\", \"-D\", \"5\", \"-n\", \"10\", \"-q\", \"0.5\", \"-G\", \"2\"])\n",
    "\n",
    "storylist = Story.from_command_list(cmds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including the integration command in our command list gives us a list of stories instead of a single story. The two parameters which govern how many stories are in the list are `-t` (the amount of dynamical time in the integration) and `-D` (the interval between snapshots). In this case, `-t` is 100, and `-D` is 5, which should give us 20 snapshots. Including initial conditions (for t=0) brings the number up to 21. The `story_from_command_list()` method automatically includes the initial conditions if the end result is a list, so we don't need to do that manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(storylist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first goal here is to make a permanent archive of this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple serialization, two ways.\n",
    "\n",
    "The simplest serialization we could use is to just write the raw output of the `starlab` commands to a file (or collection of files) in the filesystem. Then, to recreate the `Story` objects, we would have to open and read the files, and convert the strings to `Story` objects. The `Story` class has a method to do this, so it's not a problem.\n",
    "\n",
    "A second approach would be to store the `Story` objects directly on disk. The standard python way to serialize objects is the [`pickle` module](https://docs.python.org/3.5/library/pickle.html). There are some kinds of things that can't be pickled, but we're not dealing with any of them here, so using `pickle` would be very straightforward.\n",
    "\n",
    "It seems like json would be a third sensible way to approach this, but the json library doesn't know what to do with our `Story` class. There are various things we could try to convert `Story` objects to json, but it doesn't seem worth it at this point. If we were going to do direct visualization of `Story` objects via the web, it might make sense, but when we get to visualization, we'll only be using a subset of the data. In any case, we should wait until we're ready to take that step before deciding how to do it.\n",
    "\n",
    "Let's compare the two obvious options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle\n",
    "\n",
    "To use `pickle`, we have to import it first. Then we just dump our list of `Story` objects to the file. The standard extension (inasmuch as there is one) for pickle files is `.pkl`.\n",
    "\n",
    "The construction here (`with open() ... `) automatically closes the file when we're done with it, and is more concise\n",
    "(and less error prone) than opening, writing, and closing with unconnected commands would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('kiraout.pkl', 'wb') as outfile:\n",
    "    pickle.dump(storylist, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plain text\n",
    "\n",
    "We don't have to import another module to write text to a file, but we do want to make sure we're using the\n",
    "right string representation of our `Story` objects, and we want to join the list into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('kira.out', 'w') as outfile:\n",
    "    outfile.write(\"\".join([str(story) for story in storylist]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gzipped text\n",
    "\n",
    "If we're going to use text, we might as well use the `gzip` module and compress it. It adds a little computational overhead, but saves us some space. The `encode()` method is necessary to turn the string into a `bytes` object for processing. People coming from python 2 may find this unintuitive (and annoying), but ultimately I think the other benefits of python 3 make it worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "with gzip.open('kira.out.gz', 'wb') as outfile:\n",
    "    outfile.write(\"\".join([str(story) for story in storylist]).encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do they compare, size-wise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 cmckay cmckay 4770874 Feb  9 12:36 kira.out\r\n",
      "-rw-rw-r-- 1 cmckay cmckay 1425196 Feb  9 12:36 kira.out.gz\r\n",
      "-rw-rw-r-- 1 cmckay cmckay 8216885 Feb  9 12:36 kiraout.pkl\r\n"
     ]
    }
   ],
   "source": [
    "! ls -l kira*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. The pickled version is almost twice as big as the plain text, and gzipped text is the obvious winner. Since we can trivially recreate the list from the plain text file, pickling doesn't seem to buy us anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "with open('kira.out', 'r') as infile:\n",
    "    new_story_list = Story.from_buf(infile)\n",
    "    \n",
    "print(len(new_story_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filesystem vs. Database\n",
    "\n",
    "Knowing how we're going to serialize the `Story` really only answers half of the question of how we're going to archive and store our data. The other half of the question is, where are we going to put it so that we can find it again?\n",
    "\n",
    "Ultimately, we'd like to be able to find simulations that satisfy different criteria (all King model simulations with 500 stars, for example). To accomplish this, we'll need some way to connect stories with their metadata. We'll explore this in more detail in the next notebook, but it's worth going through a few options here.\n",
    "\n",
    "The biggest question, though, is whether to store our data as files in the filesystem, or as entries in a database. There are advantages and disadvantages to each, so it's not immediately obvious which is the better choice. It's the kind of thing that gets discussed occasionally in discussion groups (see, for example, [this stackoverflow thread](http://stackoverflow.com/questions/504544/whats-the-best-practice-for-storing-huge-amounts-of-text-into-a-db-or-as-a-fil)) usually without clear resolution.\n",
    "\n",
    "The main virtue of the filesystem is simplicity, but that's also its main failing. For our application, it's useful to know that:\n",
    "\n",
    "- eventually we're going to be running lots of simulations in parallel across several different machines\n",
    "- individual snapshots will typically be smaller than 1.5 MB (2600 stars gives a size of about 1.3 MB per snapshot)\n",
    "- we will be running ensembles of simulations consisting of 100 runs or so.\n",
    "- this is a write-once, read-occasionally-for-ETL situation; when we're doing analytics or plotting, we will use slices or subsets of the data rather than these archives.\n",
    "- our aim is to build a web-based interface to (at the very least) the metadata using one of the python frameworks.\n",
    "\n",
    "The last point here is probably the most important one; since we're going to be using Flask for the web interface, we'll be using SQLAlchemy for that, and it makes sense to use it for archiving the data, as well. For now, we'll put things in a SQLite database, but we'll probably use postgresql eventually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('sqlite:///test.db', echo=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQLAlchemy does two things: it's a wrapper around database accesses, and it's an Object Relational Mapper (ORM). We could build a mapping between our `Story` objects and tables in the database (and we will do that for our `Run` object) but since we're just archiving here, we can use something really simple.\n",
    "\n",
    "The following is based on the [SQLAlchemy ORM tutorial](http://docs.sqlalchemy.org/en/latest/orm/tutorial.html).\n",
    "\n",
    "We're going to use a declarative base class for our model. This provides us with some handy features (including a prebuilt `__init__()` method and methods for creating the necessary tables in the database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import Column, Integer, String, Text\n",
    "\n",
    "Base = declarative_base()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only data we're actually storing in the database (at this point) is the string representation of each `Story` object. We want to use the `Text` type rather than the `String` type because `Text` doesn't have a maximum size (though in practice it caps out at about a gigabyte). The other column, `id`, is so we can find the stories again when we need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ArchivedStory(Base):\n",
    "    __tablename__ = \"stories\"\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    story_text = Column(Text)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.story_text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we issue the command to create the table. This won't be necessary on subsequent uses, because everything will be set up already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ArchivedStory.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can store our stories, we have to convert them to the new class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "archived_stories = [ArchivedStory(story_text=str(story)) for story in storylist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the stories into the database, we need to create a session, add them, and then commit. This last step is when the actual addition happens. Again, just for emphasis: **Nothing actually happens in the database until we commit!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import sessionmaker\n",
    "Session = sessionmaker(bind=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session.add_all(archived_stories)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the first version of our serialization plan is finished. It isn't yet very useful; in particular, we don't have a connection between the `id` of any particular story and the settings that went into creating it. We'll tackle that bit next, in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Appendix:  The `Story` class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load \"../pystarlab/starlab.py\"\n",
    "from subprocess import Popen, PIPE\n",
    "import os\n",
    "import re\n",
    "from tempfile import SpooledTemporaryFile as tempfile\n",
    "\n",
    "class Story:\n",
    "    \"\"\"Generic container class for starlab data.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Create an empty story.\"\"\"\n",
    "        self.story_lines = []\n",
    "        self.story_vals = dict()\n",
    "        self.story_subobjects = []\n",
    "        self.kind = None\n",
    "        return\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"A unique representation of the story object.\"\"\"\n",
    "        return (\"[Story] %s, %d lines, %d values, %d subobjects\" %\n",
    "                (self.kind,\n",
    "                 len(self.story_lines),\n",
    "                 len(self.story_vals.keys()),\n",
    "                 len(self.story_subobjects)))\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"A string matching starlab's native format.\"\"\"\n",
    "        selfstr = \"(%s\\n\" % self.kind\n",
    "        for line in self.story_lines:\n",
    "            selfstr += \"%s\\n\" % line\n",
    "        for key, val in sorted(self.story_vals.items()):\n",
    "            selfstr += \"  %s = %s\\n\" % (key, val)\n",
    "        for substory in self.story_subobjects:\n",
    "            selfstr += str(substory)\n",
    "        return selfstr + \")%s\\n\" % self.kind\n",
    "\n",
    "    @classmethod\n",
    "    def from_buf(cls, buffered_result):\n",
    "        \"\"\"Generate a story from a buffer.\n",
    "\n",
    "        This could either be a stream or a string that has\n",
    "        been split into lines. It's supposed to add flexibility for\n",
    "        running long kira integrations in which we don't want to hold\n",
    "        the whole string in memory when converting it to stories.\n",
    "\n",
    "        We use a little bit of state to avoid using recursion here. The\n",
    "        reason for that is twofold:\n",
    "\n",
    "        1. We want to treat lines in Log-type stories a little differently, and\n",
    "        2. This will be more efficient, especially for large buffers.\n",
    "\n",
    "        :param buffered_result: Results of a starlab command in an iterable format\n",
    "        :type buffered_result: iterable\n",
    "\n",
    "        :returns: results parsed into a story\n",
    "        :rtype: story instance\n",
    "        \"\"\"\n",
    "        stories_to_return = []\n",
    "        story_stack = []\n",
    "\n",
    "        # shouldn't be necessary\n",
    "        thestory = None\n",
    "\n",
    "        for line in buffered_result:\n",
    "            if isinstance(line, bytes):\n",
    "                line = line.decode()\n",
    "            # check to see if we need to start a new story\n",
    "            storystart = re.match(\"^\\((\\w+)\",line)\n",
    "            if storystart:\n",
    "                thestory = cls()\n",
    "                thestory.kind = storystart.group(1)\n",
    "                story_stack.append(thestory)\n",
    "            else:\n",
    "                storyend = re.match(\"\\)%s\" % story_stack[-1].kind, line)\n",
    "                if storyend:\n",
    "                    thestory = story_stack.pop()\n",
    "                    if len(story_stack) > 0:\n",
    "                        story_stack[-1].story_subobjects.append(thestory)\n",
    "                    else:\n",
    "                        stories_to_return.append(thestory)\n",
    "                else:\n",
    "                    chunks = re.split('=', line)\n",
    "                    if ((len(chunks) == 2) and story_stack[-1].kind != \"Log\"):\n",
    "                        story_stack[-1].story_vals[chunks[0].strip()] = chunks[1].strip()\n",
    "                    else:\n",
    "                        story_stack[-1].story_lines.append(line)\n",
    "\n",
    "        if len(stories_to_return) == 0:\n",
    "            raise ValueError(\"No stories found in buffer!\")\n",
    "        elif len(stories_to_return) == 1:\n",
    "            return stories_to_return[0]\n",
    "        else:\n",
    "            return stories_to_return\n",
    "\n",
    "    @classmethod\n",
    "    def from_string(cls, result_string):\n",
    "        \"\"\"Generate a story from a string.\n",
    "\n",
    "        Assumes the string contains a single story (possibly with story subobjects).\n",
    "        If there's more than one story in the string (e.g., output from kira), this\n",
    "        will grab the last and discard the rest.\n",
    "\n",
    "        :param result_string: The string to parse\n",
    "        :type result_string: bytestring or unicode string\n",
    "\n",
    "        :returns: string parsed into a story\n",
    "        :rtype: Story instance\n",
    "        \"\"\"\n",
    "        if isinstance(result_string, bytes):\n",
    "            lines = result_string.decode('utf-8').splitlines()\n",
    "        elif isinstance(result_string, str):\n",
    "            lines = result_string.splitlines()\n",
    "        else:\n",
    "            raise TypeError('result_string should be a string or bytestring')\n",
    "\n",
    "        newstory = cls.from_buf(lines)\n",
    "\n",
    "        return newstory\n",
    "\n",
    "    @classmethod\n",
    "    def from_single_command(cls, command):\n",
    "        \"\"\"Generate a story from a single command.\n",
    "\n",
    "        The command should be a creation command (e.g., makeking, makeplummer, etc.).\n",
    "        It should also include all of the necessary command line options.\n",
    "\n",
    "        :param command: The starlab command to run\n",
    "        :type command: a string as it would appear on the command line\n",
    "                       or a list suitable for subprocess.Popen()\n",
    "\n",
    "        :returns: the output of command\n",
    "        :rtype: Story instance\n",
    "        \"\"\"\n",
    "        if isinstance(command, str):\n",
    "            command = command.split(\" \")\n",
    "        elif isinstance(command, list):\n",
    "            pass\n",
    "        else:\n",
    "            raise TypeError('command should be a string or list')\n",
    "        thestory = None\n",
    "        story_lines = []\n",
    "\n",
    "        with Popen(command, stdout=PIPE, bufsize=1, universal_newlines=True) as process:\n",
    "            for line in process.stdout:\n",
    "                story_lines.append(line.rstrip())\n",
    "\n",
    "        thestory = cls.from_buf(story_lines)\n",
    "\n",
    "        return thestory\n",
    "\n",
    "    @classmethod\n",
    "    def from_command_list(cls, command_list):\n",
    "        \"\"\"Generate a story from a list of commands.\"\"\"\n",
    "        current_story = cls.from_single_command(command_list.pop(0))\n",
    "        for command in command_list:\n",
    "            current_story = current_story.apply_command(command)\n",
    "        return current_story\n",
    "\n",
    "    def apply_command(self, command):\n",
    "        \"\"\"Apply a starlab command to this story and return the result\"\"\"\n",
    "        if isinstance(command, str):\n",
    "            command = command.split(\" \")\n",
    "        elif isinstance(command, list):\n",
    "            pass\n",
    "        else:\n",
    "            raise TypeError('command should be a string or list')\n",
    "\n",
    "        story_lines = []\n",
    "        with tempfile() as f:\n",
    "            f.write(str(self).encode())\n",
    "            f.seek(0)\n",
    "            with Popen(command, stdout=PIPE, stdin=f,\n",
    "                        universal_newlines=True, bufsize=1) as process:\n",
    "                for line in process.stdout:\n",
    "                    story_lines.append(line.rstrip())\n",
    "\n",
    "        thestory = self.from_buf(story_lines)\n",
    "\n",
    "        # if the command was an integration, we'll get a list\n",
    "        if isinstance(thestory, list):\n",
    "            # include the initial conditions\n",
    "            thestory.insert(0, self)\n",
    "        return thestory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
