{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with the function we defined before, to count how many stars remain after 10 dynamical times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def premain(startn):\n",
    "    \"\"\"Run a plummer model for 10 dynamical times and return the number of stars remaining.\"\"\"\n",
    "    from subprocess import Popen, PIPE\n",
    "    from starlab import parse_output, extract_particle_dynamics\n",
    "    \n",
    "    print \"running %d particles\" % startn\n",
    "    cmds = []\n",
    "\n",
    "    cmds.append([\"makeking\", \"-n\", \"%d\"%startn, \"-w\", \"5\", \"-i\",  \"-u\"])\n",
    "    cmds.append([\"makemass\", \"-f\", \"2\", \"-l\", \"0.1,\", \"-u\", \"20\"])\n",
    "    cmds.append([\"makesecondary\", \"-f\", \"0.1\", \"-l\", \"0.25\"])\n",
    "    cmds.append([\"makebinary\", \"-l\", \"1\", \"-u\", \"10\"])\n",
    "    cmds.append([\"scale\", \"-m\", \"1\", \"-e\", \"-0.25\", \"-q\", \"0.5\"]) \n",
    "    cmds.append([\"kira\", \"-t\", \"100\", \"-d\", \"1\", \"-D\", \"2\", \"-f\", \"0.3\", \"-n\", \"10\", \"-q\", \"0.5\", \"-G\", \"2\", \"-B\"])\n",
    "\n",
    "    procs = []\n",
    "    for index, cmd in enumerate(cmds):\n",
    "        print index, cmd\n",
    "        if index > 0:\n",
    "            procs.append(Popen(cmd, stdout=PIPE, stderr=PIPE, stdin=procs[index-1].stdout))\n",
    "        else:\n",
    "            procs.append(Popen(cmd, stdout=PIPE, stderr=PIPE))\n",
    "    inp = procs[-1].stdout\n",
    "    \n",
    "    result = procs[-1].communicate()\n",
    "    slist = parse_output(result[0])\n",
    "    return len(extract_particle_dynamics(slist[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to run this in parallel. To do so, we need to spin up a set of compute engines. Go to the IPython dashboard (where you see the collection of notebooks) and click on the clusters tab. Find the row with your name, type 50 in the box, and hit the start button.  Actually, the way things are set up, it doesn't matter what you type in the box, all of the engines assigned to you will start up.  To find out how many you've actually got, we need to set up a way to interact with the cluster (called a \"Client\").  The client keeps a list of all of the compute engines; by finding the length of this list, we will know how many engines we have access to.\n",
    "\n",
    "The client needs to know which cluster profile to use. You'll supply your last name as a `profile` keyword when you set up the Client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.parallel import Client\n",
    "rc = Client(profile='Bragg')\n",
    "len(rc.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to actually run jobs on the cluster, we construct what's called a view. We can either use a \"direct view\", which sends specific tasks to specific engines, or we can use a \"load balanced view\", which takes care of assignment for us. At least initially, we'll go with this latter option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lbv = rc.load_balanced_view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example of how this all works, let's run a set of 500 simulations, each with 100 stars.  We first make a list that contains the number of stars (100) 500 times. Then we `map` the function we defined above on to that list, calling it once for each of the items in the list. We could just as easily build a list that had a variety of numbers in it, but since we want to look at variations in results given the same input, we don't want to do that just yet. We don't have to wait for results before we move on to other things (i.e., we can let it run without tying up our notebook) but for this short example, we'll keep an eye on how things are progressing by using the `wait_interactive()` function.\n",
    "\n",
    "Note that if you try to do a large number of jobs (more than 1000 or so), the `map_async()` takes a longish time to actually queue all of the jobs, but they start running as soon as the first one gets queued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thepoints = [100] * 80\n",
    "results = lbv.map_async(premain, thepoints)\n",
    "results.wait_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see the results, we need to fire up some plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll put the results into a `numpy` array (which is a little easier to deal with than a list) and then plot a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = np.array(results)\n",
    "plt.hist(res, res.max() - res.min())\n",
    "plt.xlabel(\"Number of stars remaining\")\n",
    "plt.ylabel(\"Number of runs\")\n",
    "plt.title(r\"Stars remaining after 100 dynamical times, $N_0 = 100$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look about as close to Gaussian as you're going to get with only 500 points.\n",
    "\n",
    "That's basic parallel use. When you're done with your computations, it's a good idea to stop the compute engines: go back to the cluster tab on the dashboard and hit the appropriate stop button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for kicks, let's see how it looks with 5000 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thepoints = [100] * 5000\n",
    "results = lbv.map_async(premain, thepoints)\n",
    "results.wait_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = np.array(results)\n",
    "#plt.figsize(10,10) # make the figure a little bigger so we can see better\n",
    "plt.hist(res, res.max() - res.min()) # arguments are the results array and the number of bins to use for the histogram\n",
    "plt.xlabel(\"Number of stars remaining\")\n",
    "plt.ylabel(\"Number of runs\")\n",
    "plt.title(r\"Stars remaining after 100 dynamical times, $N_0 = 100$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
